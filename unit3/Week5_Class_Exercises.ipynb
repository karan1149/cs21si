{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5_Class_Exercises.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Iwim91a7hs3Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Week 5 Exercises: COMPAS\n",
        "\n",
        "Today, we'll be exploring deep neural networks and applying them to data related to the COMPAS algorithm. I got this data [here](https://github.com/propublica/compas-analysis), in case you're interested in exploring it on your own!"
      ]
    },
    {
      "metadata": {
        "id": "KMOKWAjMhs3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up the Environment\n",
        "\n",
        "Run any code below by highlighting it and hitting `Shift + Enter`. Import the libraries below."
      ]
    },
    {
      "metadata": {
        "id": "xVVtEpUxhs3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Fix random seed for reproducibility.\n",
        "np.random.seed(1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xn944nQghs3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read in Data\n",
        "\n",
        "Run the below code to load the data and select the columns we care about."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "YfiR8vPmhs3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download and extract data.\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit3_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "data = pd.read_csv(\"unit3_resources/compas-scores.csv\", header = 0)\n",
        "\n",
        "# Select fields we want.\n",
        "fields_of_interest = ['name', 'sex', 'age', 'race', 'priors_count', 'c_charge_desc', \n",
        "                      'v_decile_score', 'decile_score', 'is_violent_recid', 'is_recid']\n",
        "data = data[fields_of_interest]\n",
        "\n",
        "# More interpretable column names.\n",
        "data.columns = ['name', 'sex', 'age', 'race', 'num_priors', 'charge', \n",
        "                'violence_score', 'recidivism_score', 'violence_true', 'recidivism_true']\n",
        "\n",
        "# Remove records with missing scores.\n",
        "data = data.loc[(data.violence_score != -1) & (data.recidivism_score != -1)]\n",
        "data = data.loc[(data.violence_true != -1) & (data.recidivism_true != -1)]\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KX0FJEOmhs3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Understanding the columns: *num_priors* refers to the number of prior offenses, *violence_score* and *recividism_score* refer to scores predicted by the COMPAS algorithm, and *violence_true* and *recividism_true* refer to the groundtruth (i.e., did this person commit another crime).\n",
        "\n",
        "## Part 1: Visualize the Data\n",
        "\n",
        "With the increasing availability of rich data sets encoding several features, it's difficult to extract useful knowledge just by looking at the numbers. This is where data visualizations come in. "
      ]
    },
    {
      "metadata": {
        "id": "2RNXrDqWhs3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# races of interest\n",
        "races = ['African-American', 'Caucasian', 'Hispanic']\n",
        "colors = ['magenta', 'yellow', 'cyan']\n",
        "\n",
        "#########################################################\n",
        "# Counts number of people per race. Plots as bar graph.\n",
        "#########################################################\n",
        "def plot_racial_distrib():\n",
        "    race_indices = [1,2,3]\n",
        "    race_population = [(data.loc[data['race'] == race].shape[0]) for race in races]\n",
        "\n",
        "    plt.bar(race_indices, race_population, align='center', color = colors)\n",
        "    plt.xticks(race_indices, races)\n",
        "    plt.title('Racial Distribution of COMPAS Defendants')\n",
        "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
        "    plt.show()\n",
        "\n",
        "#########################################################\n",
        "# Plots input feature by race as bar graph.\n",
        "#########################################################\n",
        "def plot_feature_by_race(feature, normalized = True):\n",
        "    # bar chart parameters\n",
        "    width = 0.25\n",
        "    groups = sorted(data[feature].unique(), key=int)\n",
        "    deciles = np.arange(1, len(groups) + 1) # for each of the 10 scores\n",
        "    \n",
        "    if normalized:\n",
        "        race_population = [(data.loc[data['race'] == race].shape[0]) for race in races]\n",
        "    else:\n",
        "        race_population = [1 for race in races]\n",
        "    \n",
        "    race_bars = []\n",
        "    for i in range(len(races)):\n",
        "        race_data = data.loc[data['race'] == races[i]]\n",
        "        bar = plt.bar(deciles + (i-1)*width, race_data[feature].value_counts()/race_population[i], \n",
        "                      width, color = colors[i])\n",
        "        race_bars.append(bar)\n",
        "\n",
        "    plt.title('COMPAS ' + feature + ' by Race')\n",
        "    plt.xticks(deciles + width / 2, groups)\n",
        "    plt.legend(tuple(bar for bar in race_bars), tuple(races))\n",
        "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
        "\n",
        "    plt.show()\n",
        "    plt.clf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZBiv32_hs3k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your task:** Using the functions defined above, plot the racial distribution of defendants in our database. Also plot the violence and recidivism COMPAS scores by race to produce three total plots. You should see that African-Americans are disproportionately assigned higher recidivism and violence scores, compared to Caucasians and Hispanics."
      ]
    },
    {
      "metadata": {
        "id": "I3vc2a1fhs3l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE:\n",
        "\n",
        "\n",
        "\n",
        "# END CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKWB66f9hs3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean up the Data a Little More\n",
        "\n",
        "Just like in week 3, we want to convert our data from a human-readable format to a format more amenable for a neural network. We need to convert text in our data into meaningful numbers. For instance, below we map \"Male\" to 0 and \"Female\" to 1. Note that for `race`, we want to use a one-hot encoding again, just as in week 3."
      ]
    },
    {
      "metadata": {
        "id": "wQbv0Ot5hs3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert strings to numerical values.\n",
        "sex_classes = {'Male': 0, 'Female' : 1}\n",
        "\n",
        "processed_data = data.copy()\n",
        "processed_data['sex'] = data['sex'].apply(lambda x: sex_classes[x])\n",
        "\n",
        "# One-hot encode race.\n",
        "processed_data = pd.get_dummies(processed_data, columns = ['race'])\n",
        "columns = processed_data.columns.tolist()\n",
        "columns = columns[0:3] + columns[9:] + columns[3:9]\n",
        "processed_data = processed_data.reindex(columns = columns)\n",
        "\n",
        "processed_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q5Nv4Qfmc3PX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert Pandas dataframe to NumPy array for easier processing.\n",
        "processed_data = processed_data.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oN9r98dhs3s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: Partition into Train and Test Sets\n",
        "\n",
        "We will focus on predicting recidivism scores from defendant data and recidivism ground truth. This is now a binary classification problem where our inputs are sex, age, race, and number of prior convictions. Our output is 1 (high risk of recidivism) or 0 (low risk of recidivism). The entire dataset has already been split into inputs (X) and outputs (y). \n"
      ]
    },
    {
      "metadata": {
        "id": "0mTabcx4hs3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split into input (X) and output (y) variables.\n",
        "X = processed_data[:,1:10] # sex, age, race, num_priors\n",
        "y = processed_data[:,14] # recidivism_true\n",
        "\n",
        "num_train = int(math.ceil(X.shape[0]*0.8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AmIfELB718G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your task:** Partition the dataset into two sets: train (80%) and test (20%). The validation split will happen later automatically (using Keras). Assume the records have already been shuffled."
      ]
    },
    {
      "metadata": {
        "id": "x7n1uIWkhs3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# END CODE\n",
        "\n",
        "num_classes = 2\n",
        "# Convert output classes to one-hot encodings. As a result,\n",
        "# we use softmax instead of sigmoid later.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(X_train.shape[0], 'records in train set')\n",
        "print(X_test.shape[0], 'records in test set')\n",
        "print(X.shape[0], 'records in total')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9BPoZuVqhs3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3: Create a Baseline Model\n",
        "\n",
        "**Your task:** At this point, we're ready to create a basic neural network classifier using Keras. Check this [guide](https://keras.io/getting-started/sequential-model-guide/) out to get started with Keras. Your model should have 4 Dense (fully-connected) layers, of sizes 50, 100, 50, and `num_classes`. All layers, save the last one, should use a ReLU activation function. The last layer should use softmax (why not sigmoid, since this is binary classification?). Make sure you pass in the *input_dim* parameter for the first layer, to tell Keras what size the input vectors are."
      ]
    },
    {
      "metadata": {
        "id": "YQqkuj1whs3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 25\n",
        "learning_rate = 1e-5\n",
        "\n",
        "#########################################################\n",
        "# Initializes baseline neural network with 4 layers.\n",
        "# ReLU and Softmax activations. Cross-entropy loss.\n",
        "#########################################################\n",
        "def baseline_classifier(learning_rate):\n",
        "    # Create sequential model.\n",
        "    model_baseline = Sequential()\n",
        "\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    # END CODE\n",
        "\n",
        "    # Compile model.\n",
        "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
        "    model_baseline.compile(loss = keras.losses.categorical_crossentropy, \n",
        "                  optimizer = sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model_baseline\n",
        "\n",
        "model_baseline = baseline_classifier(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMJ69xm_hs32",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4: Train and Evaluate the Baseline Model\n",
        "\n",
        "**Your task:** Use the `eval()` function below to train and evaluate our baseline model. The return value of `eval()` is a tuple of loss and accuracy. Print both of these. In `eval()`, feel free to change the value of the `verbose` parameter. When `verbose = 0`, no information is printed. When it's 5, a lot of detailed information about the training process gets printed. Your test accuracy for this basic model should be around 69%.\n",
        "\n",
        "Note that a parameter is `model.fit()` is `validation_split`. This takes a float from 0 to 1, representing the percentage of the training set to use for validation. Why do we need a validation set? More than training performance, we are interested in how our model does on unseen data. We can split data into only training and test. But if we then optimize our model using results from the test set, our test set can no longer be considered unseen data. As a workaround, we split our dataset into train, validation, and test. This way, we can optimize on our validation set, and only touch our test set at the very end."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "e7-orFaEhs32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "# Trains and evaluates given model. Returns loss and \n",
        "# accuracy.\n",
        "#########################################################\n",
        "def eval(model, verb = 2):\n",
        "    # Fit the model.\n",
        "    model.fit(X_train, y_train, \n",
        "              epochs = epochs, \n",
        "              batch_size = batch_size,          \n",
        "              validation_split = 0.1,\n",
        "              verbose = verb,\n",
        "              shuffle = False)\n",
        "    \n",
        "    # Evaluate the model.\n",
        "    scores = model.evaluate(X_test, y_test)\n",
        "    \n",
        "    return scores\n",
        "\n",
        "# YOUR CODE HERE:\n",
        "loss, acc = None\n",
        "# END CODE\n",
        "print('\\n\\nTest loss:', loss)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MncrddAWhs34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 5: Introduce Regularization\n",
        "\n",
        "Above, you might have noticed that your train accuracies were consistently higher than your test accuracy.  Another issue is that the train losses continue to decrease while the validation losses hit a minimum then increase. Both of these things are indicators of overfitting. Right now, our model doesn't generalize as well as we want. Regularization is a way to fix this. Regularization reduces overfitting by adding a penalty to the loss function. By adding this penalty, the model is trained such that it does not learn interdependent sets of features weights.\n",
        "\n",
        "**Your task:** Add L2 regularization to each hidden layer of our model. You may want to refer to the [Keras documentation about regularizers](https://keras.io/regularizers/). You may see up to a 5% gain in test accuracy after adding in regularization."
      ]
    },
    {
      "metadata": {
        "id": "Cq77HWSzhs35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reg_strength = 5e-4\n",
        "\n",
        "#########################################################\n",
        "# Initializes neural network with L2 regularization.\n",
        "#########################################################\n",
        "def regularized_classifier(learning_rate, reg_strength):\n",
        "    # create model\n",
        "    model_regularized = Sequential()\n",
        "\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    # END CODE\n",
        "    \n",
        "    # compile model\n",
        "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
        "    model_regularized.compile(loss = keras.losses.categorical_crossentropy, \n",
        "                  optimizer = sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model_regularized\n",
        "\n",
        "for reg_strength in [1e-1, 1e-2, 1e-3, 1e-4]:\n",
        "  print()\n",
        "  print(\"Regularization strength: %f\" % reg_strength)\n",
        "  model_regularized = regularized_classifier(learning_rate, reg_strength)\n",
        "\n",
        "  loss, acc = eval(model_regularized, verb = 0)\n",
        "  print('\\n\\nTest loss:', loss)\n",
        "  print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGlZDQBG_7Wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How does this compare with COMPAS?\n",
        "\n",
        "At this point, you've trained a model using defendant survey data and recidivism ground truth, and tried to improve your model with regularization. Still, you should see that from the first plot, African-Americans are predicted to be more likely to recidivate, compared to Caucasians and Hispanics after normalizing for population size. \n",
        "\n",
        "This tells us a few things. First, even with a sound machine learning pipeline, it's possible to get a biased model. Secondly, biased models are largely a result of biased data. If we look at the racial distribution of the COMPAS dataset again, we see that of recidivating defendants, African-Americans are the clear plurality. Our dataset just has more instances of recidivating African-Americans (see plot 2), which is either a result of biased selection by the creators of the dataset or racial disparities in policing in the US."
      ]
    },
    {
      "metadata": {
        "id": "5h2JimJf-n9M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use model to make a prediction for every defendant.\n",
        "y_model = model_regularized.predict_classes(X)\n",
        "data['model_prediction'] = y_model\n",
        "\n",
        "plot_feature_by_race('model_prediction')\n",
        "plot_feature_by_race('recidivism_true')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGCFL_kMhs37",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Bonus) Part 6: Introduce Dropout\n",
        "\n",
        "Dropout is another regularization technique to prevent over-fitting. Dropout randomly selects neurons to ignore during training. In other words, they are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. \n",
        "\n",
        "**Your task:** Remove the regularization, and this time around, add dropout for the input layer. You may want to refer to the [Keras documentation about the Dropout layer](https://keras.io/layers/core/). You may see up to a 5% increase in test accuracy compared to the regularization model."
      ]
    },
    {
      "metadata": {
        "id": "kukz51U8hs37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dropout_strength = 1e-2\n",
        "\n",
        "#########################################################\n",
        "# Initializes neural network with dropout.\n",
        "#########################################################\n",
        "def dropout_classifier(learning_rate, dropout_strength):\n",
        "    # create model\n",
        "    model_dropout = Sequential()\n",
        "\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # END CODE\n",
        "    \n",
        "    # compile model\n",
        "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
        "    model_dropout.compile(loss = keras.losses.categorical_crossentropy, \n",
        "                  optimizer = sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model_dropout\n",
        "\n",
        "model_dropout = dropout_classifier(learning_rate, dropout_strength)\n",
        "\n",
        "loss, acc = eval(model_dropout, verb = 0)\n",
        "print('\\n\\nTest loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxjHGXx3hs3-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Bonus) Part 7: Introduce Batch Normalization\n",
        "\n",
        "Batch Normalization is used to help remedy “covariate shifts”, or changes in the distribution of function’s domain.\n",
        "They help keep activations in the zero-mean unit-variance range. You might use batch normalization for the same reasons that you normalized the entire dataset initially.\n",
        "\n",
        "**Your task:** For our final exercise, keep dropout. Add batch normalization after the first dense layer. You may want to refer to the [Keras documentation about the BatchNorm layer](https://keras.io/layers/normalization/). You may see up to a 5% increase in test accuracy compared to the dropout-only model."
      ]
    },
    {
      "metadata": {
        "id": "oKZ24Syuhs3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "# Initializes neural network with dropout.\n",
        "#########################################################\n",
        "def batchnorm_classifier(learning_rate, dropout_strength):\n",
        "    # create model\n",
        "    model_batchnorm = Sequential()\n",
        "\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # END CODE\n",
        "    \n",
        "    # compile model\n",
        "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
        "    model_batchnorm.compile(loss = keras.losses.categorical_crossentropy, \n",
        "                  optimizer = sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model_batchnorm\n",
        "\n",
        "model_batchnorm = batchnorm_classifier(learning_rate, dropout_strength)\n",
        "\n",
        "loss, acc = eval(model_batchnorm, verb = 0)\n",
        "print('\\n\\nTest loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}