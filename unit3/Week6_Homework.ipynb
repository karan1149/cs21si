{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6_Homework_Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Km5-9QmN8Rl",
        "colab_type": "text"
      },
      "source": [
        "# Week 6 Homework: Refining Our Notion of Algorithmic Bias For COMPAS\n",
        "\n",
        "In this notebook, we will continue our exploration of the COMPAS dataset. We'll more closely examine what it means for an algorithm to be fair using the [What-If Tool](https://pair-code.github.io/what-if-tool/index.html), a visual interface for probing machine learning models produced by the PAIR group at Google AI. \n",
        "\n",
        "Before starting this notebook, read [this article](https://pair-code.github.io/what-if-tool/ai-fairness.html) introducing the tool in context of predicting loan risk. The article describes five different notions of algorithmic fairness that will come up in our analysis.\n",
        "\n",
        "After reading the article, run the below cell to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1iTa5-6N4bF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import google.colab\n",
        "!pip install --upgrade witwidget\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import functools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8DIQSZASZH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to import helpers { display-mode: \"form\" }\n",
        "\n",
        "# Creates a tf feature spec from the dataframe and columns specified.\n",
        "def create_feature_spec(df, columns=None):\n",
        "    feature_spec = {}\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for f in columns:\n",
        "        if df[f].dtype is np.dtype(np.int64):\n",
        "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.int64)\n",
        "        elif df[f].dtype is np.dtype(np.float64):\n",
        "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.float32)\n",
        "        else:\n",
        "            feature_spec[f] = tf.FixedLenFeature(shape=(), dtype=tf.string)\n",
        "    return feature_spec\n",
        "\n",
        "# Creates simple numeric and categorical feature columns from a feature spec and a\n",
        "# list of columns from that spec to use.\n",
        "#\n",
        "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
        "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
        "def create_feature_columns(columns, feature_spec):\n",
        "    ret = []\n",
        "    for col in columns:\n",
        "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
        "            ret.append(tf.feature_column.numeric_column(col))\n",
        "        else:\n",
        "            ret.append(tf.feature_column.indicator_column(\n",
        "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
        "    return ret\n",
        "\n",
        "# An input function for providing input to a model from tf.Examples\n",
        "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
        "                       num_epochs=None, \n",
        "                       batch_size=64):\n",
        "    def ex_generator():\n",
        "        for i in range(len(examples)):\n",
        "            yield examples[i].SerializeToString()\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    return dataset\n",
        "\n",
        "# Parses Tf.Example protos into features for the input function.\n",
        "def parse_tf_example(example_proto, label, feature_spec):\n",
        "    parsed_features = tf.parse_example(serialized=example_proto, features=feature_spec)\n",
        "    target = parsed_features.pop(label)\n",
        "    return parsed_features, target\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "    examples = []\n",
        "    if columns == None:\n",
        "        columns = df.columns.values.tolist()\n",
        "    for index, row in df.iterrows():\n",
        "        example = tf.train.Example()\n",
        "        for col in columns:\n",
        "            if df[col].dtype is np.dtype(np.int64):\n",
        "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "            elif df[col].dtype is np.dtype(np.float64):\n",
        "                example.features.feature[col].float_list.value.append(row[col])\n",
        "            elif row[col] == row[col]:\n",
        "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "        examples.append(example)\n",
        "    return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RS0F-5fDITV",
        "colab_type": "text"
      },
      "source": [
        "Run the below cell to load the COMPAS data. Note that the column names aren't exactly the same as before because we are using a different data source. Some columns to highlight: *recidivism_within_2_years* is the groundtruth recividism, *decile_score* is the predicted recividism score according to COMPAS, and *COMPAS_determination* is a binary number representing whether the predicted score is low (0) or not (1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py11h6HY9KOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://storage.googleapis.com/what-if-tool-resources/computefest2019/cox-violent-parsed_filt.csv')\n",
        "df = df.drop(columns=['id', 'dob', 'screening_date', 'age_cat', 'event', 'v_type_of_assessment', 'v_decile_score', 'v_score_text', 'r_jail_in', 'vr_charge_degree', 'vr_offense_date', 'vr_charge_desc', 'r_charge_degree', 'r_days_from_arrest', 'r_offense_date', 'r_charge_desc', 'c_days_from_compas', 'c_charge_degree',\t'c_charge_desc', 'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'violent_recid', 'is_violent_recid', 'type_of_assessment', 'decile_score.1', 'priors_count.1'])\n",
        "\n",
        "# Filter out entries with no indication of recidivism or no compass score\n",
        "df = df[df['is_recid'] != -1]\n",
        "df = df[df['decile_score'] != -1]\n",
        "\n",
        "# Rename recidivism column\n",
        "df['recidivism_within_2_years'] = df['is_recid']\n",
        "\n",
        "# Make the COMPAS label column numeric (0 and 1), for use in our model\n",
        "df['COMPAS_determination'] = np.where(df['score_text'] == 'Low', 0, 1)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhGrFG4_Hrui",
        "colab_type": "text"
      },
      "source": [
        "We will now train a classifier to predict the *COMPAS_determination* field. In other words, we are training a classifier to replicate COMPAS rather than predict the groundtruth (as we did last week). We do this with the goal of better understanding the original COMPAS model that we are replicating!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGI29ZJg9Qix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set column to predict\n",
        "label_column = 'COMPAS_determination'\n",
        "\n",
        "# Get list of all columns from the dataset we will use for model input or output.\n",
        "input_features = ['sex', 'age', 'race', 'priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count']\n",
        "features_and_labels = input_features + [label_column]\n",
        "\n",
        "features_for_file = input_features + ['recidivism_within_2_years', 'COMPAS_determination']\n",
        "\n",
        "examples = df_to_examples(df, features_for_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYjJm-3y9dUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Create and train the classifier (run this cell!) {display-mode: \"form\"}\n",
        "\n",
        "num_steps = 2000  #@param {type: \"number\"}\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "\n",
        "# Create a feature spec for the classifier\n",
        "feature_spec = create_feature_spec(df, features_and_labels)\n",
        "\n",
        "# Define and train the classifier\n",
        "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n",
        "classifier = tf.estimator.LinearClassifier(\n",
        "    feature_columns=create_feature_columns(input_features, feature_spec))\n",
        "classifier.train(train_inpf, steps=num_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozAUoV_H0mkq",
        "colab_type": "text"
      },
      "source": [
        "Run the below cell to launch the What-If Tool. Note that this tool works best when using Google Chrome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPU8hO8ABsfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this to launch What-If Tool for test data and the trained models {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "num_datapoints = 10000\n",
        "tool_height_in_px = 700\n",
        "\n",
        "from witwidget.notebook.visualization import WitConfigBuilder\n",
        "from witwidget.notebook.visualization import WitWidget\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[0:num_datapoints]).set_estimator_and_feature_spec(\n",
        "    classifier, feature_spec)\n",
        "WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlFSiTe5Jgy5",
        "colab_type": "text"
      },
      "source": [
        "# Exploration\n",
        "\n",
        "Once you've launched the What-If Tool, begin by exploring the \"Datapoint editor\" tab (on the top-left), which enables you to visualize datapoints and even modify individual datapoints. Try clicking on one of the red or blue dots on the right panel to examine the training example. You are able to view the features for the training example, the groundtruth (*recividism_within_2_years*), and the inference value (predicted by the model). You can modify a feature and click the \"Run inference\" button to see the effect it had on the model's prediction. What happens if you change an example's race from \"Caucasian\" to \"African-American\"? To \"Asian\"? What happens if you change age and sex? Put your answers to these questions in the below cell, in a few sentences. If you like, you can also play around with the visualization tool on the right panel to explore different ways of slicing and presenting the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqEO18E_KNag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "YOUR ANSWERS HERE\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e75Ne57W3Kvg",
        "colab_type": "text"
      },
      "source": [
        "After you've explored the \"Datapoint editor\" tab, click on the \"Features\" tab (near the top). This view gives you distributions for the features for each example–another useful way to better understand your data. We can see that certain demographics are overrepresented in our data (e.g., African-Americans and males). We can also see that about 25% of examples have 0 prior offenses (*prior_count*). \n",
        "\n",
        "Finally, click on the \"Performance and Fairness\" tab at the top of the tool. On the left, under \"Ground Truth Feature\", select *recividism_within_2_years*. Leave the cost ratio at 1. This sets the threshold for binary classification (which we have previously set to 0.5) such that the ratio of false positives to false negatives is 1. Effectively, we are telling our model that false positives are just as bad as false negatives, and the model sets the threshold for binary classification so that this is the case. Given the threshold, if the model predicts a value lower than the threshold, we say that recividism risk is low, otherwise the risk is not low. Note that in the past we have arbitrarily used 0.5 as the threshold, but this does not necessarily lead to a ratio of false positives to false negatives of 1. We can imagine for this problem a cost ratio of 1 may be undesirable, but for consistency we leave this at 1 for now (feel free to play around with this later and see the effect on the thresholds). Lastly, under \"Slice by\", choose \"race\".\n",
        "\n",
        "Now you are able to view performance information for each race. Click \"Single threshold\" to start. The threshold value (0.64) is set so that the overall cost ratio is 1, but you'll notice that different races have different ratios of false positives to false negatives. In particular, African-Americans have a higher false positive rate than other races. \n",
        "\n",
        "Note that \"Single threshold\" corresponds most closely to \"Group unaware\" from the reading, although in the \"Single threshold\" case, race is still used as a feature, even if thresholds are the same for all races. Otherwise, the other notions of algorithmic fairness are presented as options.\n",
        "\n",
        "Play around with the different options, noticing how the thresholds, rates of false positives and false negatives, and accuracies change for each.\n",
        "\n",
        "Given how the thresholds are different for each option, you might suspect that it is impossible to satisfy all of these notions of fairness at the same time. In fact, researchers [have found that on realistic data](https://arxiv.org/pdf/1609.05807.pdf), this intuition is borne out in practice.\n",
        "\n",
        "Given that we cannot fulfill all notions of algorithmic fairness at once for the problem of predicting recividism, which would you choose? In the below cell, pick one of the following, and defend your choice in a few sentences. There is no right answer! Be sure to reference figures from the What-If Tool in your analysis.\n",
        "\n",
        "1.   Group unaware\n",
        "2.   Group thresholds\n",
        "3.   Demographic parity\n",
        "4.   Equal opportunity\n",
        "5.   Equal accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBFsXmi23J4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "YOUR ANSWER HERE\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRw4c7hXSZyh",
        "colab_type": "text"
      },
      "source": [
        "# Acknowledgements\n",
        "\n",
        "This notebook is adapted from [code from the PAIR group at Google](https://github.com/PAIR-code/what-if-tool/blob/master/WIT_COMPAS.ipynb). If you liked this homework, you can use the What-If Tool to [perform a similar analysis on an income classification model built on top of Census Bureau data](https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_Model_Comparison.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdCcc4dsTErS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}