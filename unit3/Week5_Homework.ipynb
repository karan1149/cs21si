{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5_Homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "scuMd1ZehWhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Week 5 Homework: Refining our COMPAS Model\n",
        "\n",
        "This week's homework is focused on making further improvements to our COMPAS-like deep neural network. We will be performing hyperparameter tuning. As discussed during lecture, you might not always see huge improvements from regularization. Oftentimes, this is because **hyperparameters** including regularization strengths are not optimal. Hyperparameter tuning trains several different models, each with a different combinations of hyperparameter values."
      ]
    },
    {
      "metadata": {
        "id": "lfI_AgzNhWhJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up the Environment\n",
        "\n",
        "Run any code below by highlighting it and hitting `Shift + Enter`. Import the libraries below."
      ]
    },
    {
      "metadata": {
        "id": "4j1O0r-ZhWhJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Fix random seed for reproducibility.\n",
        "np.random.seed(1337)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7KrUkjZhWhO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read and Clean in Data"
      ]
    },
    {
      "metadata": {
        "id": "v7OGQX7jhWhP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download and extract data.\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit3_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "data = pd.read_csv(\"unit3_resources/compas-scores.csv\", header = 0)\n",
        "\n",
        "# Select fields we want.\n",
        "fields_of_interest = ['name', 'sex', 'age', 'race', 'priors_count', 'c_charge_desc', \n",
        "                      'v_decile_score', 'decile_score', 'is_violent_recid', 'is_recid']\n",
        "data = data[fields_of_interest]\n",
        "# More interpretable column names.\n",
        "data.columns = ['name', 'sex', 'age', 'race', 'num_priors', 'charge', \n",
        "                'violence_score', 'recidivism_score', 'violence_true', 'recidivism_true']\n",
        "\n",
        "# Remove records with missing scores.\n",
        "data = data.loc[(data.violence_score != -1) & (data.recidivism_score != -1)]\n",
        "data = data.loc[(data.violence_true != -1) & (data.recidivism_true != -1)]\n",
        "\n",
        "# Convert strings to numerical values.\n",
        "sex_classes = {'Male': 0, 'Female' : 1}\n",
        "\n",
        "processed_data = data.copy()\n",
        "processed_data['sex'] = data['sex'].apply(lambda x: sex_classes[x])\n",
        "\n",
        "# One-hot encode race.\n",
        "processed_data = pd.get_dummies(processed_data, columns = ['race'])\n",
        "columns = processed_data.columns.tolist()\n",
        "columns = columns[0:3] + columns[9:] + columns[3:9]\n",
        "processed_data = processed_data.reindex(columns = columns)\n",
        "\n",
        "processed_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xJrhaF1wThxm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert pandas dataframe to numpy array for easier processing.\n",
        "processed_data = processed_data.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "996P_v06hWhR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Partition into Train and Test Sets\n",
        "\n",
        "This was all code you wrote during lecture, so we've given it to you as a freebie here!"
      ]
    },
    {
      "metadata": {
        "id": "HvNKjloYhWhS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split into input (X) and output (Y) variables\n",
        "X = processed_data[:,1:10] # sex, age, race, num_priors\n",
        "y = processed_data[:,14] # recidivism_true\n",
        "\n",
        "num_train = int(math.ceil(X.shape[0]*0.8))\n",
        "num_test = int(math.floor(X.shape[0]*0.2))\n",
        "\n",
        "X_train = X[:num_train]\n",
        "y_train = y[:num_train]\n",
        "\n",
        "X_test = X[num_train:]\n",
        "y_test = y[num_train:]\n",
        "\n",
        "num_classes = 2\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(X_train.shape[0], 'records in train set')\n",
        "print(X_test.shape[0], 'records in test set')\n",
        "print(X.shape[0], 'records in total')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZIHnl6KhWhU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up our Evaluation Pipeline\n",
        "\n",
        "Again, we wrote this together in class, so it's given to you here."
      ]
    },
    {
      "metadata": {
        "id": "FYVcQZB5hWhV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "# Trains and evaluates given model. Returns loss and \n",
        "# accuracy.\n",
        "#########################################################\n",
        "def eval(model, verb = 2):\n",
        "    # fit the model\n",
        "    model.fit(X_train, y_train, \n",
        "              epochs = 30, \n",
        "              batch_size = batch_size,          \n",
        "              validation_split = 0.1,\n",
        "              verbose = verb,\n",
        "              shuffle = False)\n",
        "    \n",
        "    # Evaluate the model.\n",
        "    scores = model.evaluate(X_test, y_test)\n",
        "    \n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzh9RskXhWhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1: Create and Evaluate your Own Classifier\n",
        "\n",
        "During lecture, you learned about adding `Dense` layers (which are just fully-connected layers that can have the parameter `kernel_regularizer`) to a Keras model. Check out this [guide](https://keras.io/getting-started/sequential-model-guide/) for some good examples in case you want a refresher.\n",
        "\n",
        "**Your task:** Now that you've gotten your feet wet with Keras, in this exercise, you get to decide how you want to set up your model! Add layers of any size (paired with any activation functions) as you see fit, and if you want to experiment with other Keras add-ons that we haven't discussed before like `Dropout` or `BatchNormalization`, then go for it. Similarly, if you elect not to use L2-regularization, then that's up to you too. Good luck!"
      ]
    },
    {
      "metadata": {
        "id": "njnnNithhWhX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_classes = 2\n",
        "\n",
        "learning_rate = 2e-3\n",
        "reg_strength = 1e-4\n",
        "\n",
        "#########################################################\n",
        "# Initializes neural network with dropout.\n",
        "#########################################################\n",
        "def nn_classifier(learning_rate, reg_strength, dropout_strength=0.5):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    # END CODE\n",
        "    \n",
        "    # compile model\n",
        "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
        "    model.compile(loss = keras.losses.categorical_crossentropy, \n",
        "                  optimizer = sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Evaluate your model\n",
        "for learning_rate in [1e-2]:\n",
        "  for reg_strength in [1e-4]:\n",
        "    print(\"Using learning rate %f and regularization strength %f...\" % (learning_rate, reg_strength))\n",
        "    model = nn_classifier(learning_rate, reg_strength)\n",
        "    loss, acc = eval(model, verb = 2)\n",
        "    print('\\n\\nTest loss:', loss)\n",
        "    print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LUEKHCydhWha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2: Hyperparameter Tuning\n",
        "\n",
        "For the purposes of playing around with hyperparameter tuning and understanding the motivations behind it, we're going to be working with a naive implementation below.  \n",
        "\n",
        "**Your task**: Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using neural networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including learning rate and regularization strength. Your goal in this exercise is to get as good of a result on the COMPAS dataset as you can, with a fully-connected deep neural network. Feel free to change the model you initialized above in Exercise 1 as well. The starter code below is there to give you a naive implementation of hyperparameter tuning. You can change it as you wish. **Challenge yourself to hit at least 69% validation accuracy with your best model.** Good luck!"
      ]
    },
    {
      "metadata": {
        "id": "EEnwCgsZhWha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tune_hyperparams():\n",
        "    best_model = (None, None, None)\n",
        "    running_best_accuracy = 0\n",
        "\n",
        "    # Play with these!\n",
        "    learning_rate = [1e-2, 1e-3]\n",
        "    reg_strength = [1e-3, 1e-4] \n",
        "    \n",
        "    for lr in learning_rate:\n",
        "        for reg in reg_strength:\n",
        "            model = nn_classifier(lr, reg)\n",
        "            model_loss, model_acc = eval(model, verb = 0)\n",
        "\n",
        "            print('\\n val_acc: {:f}, lr: {:f}, reg: {:f}\\n'.format(\n",
        "                    model_acc, lr, reg))\n",
        "\n",
        "            if model_acc > running_best_accuracy:\n",
        "                model_params = {\"lr\": lr, \"reg\": reg}\n",
        "                best_model = (model, model_acc, model_params)\n",
        "                running_best_accuracy = model_acc\n",
        "            \n",
        "    return best_model\n",
        "        \n",
        "best_model = tune_hyperparams()\n",
        "print(\"\\n\\nBest Model Performance: \", best_model[1])\n",
        "print(\"Hyperparameters of Best Model: \", best_model[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTf7YYu0tNrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}