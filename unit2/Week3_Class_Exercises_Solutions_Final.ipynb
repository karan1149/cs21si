{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week3_Class_Exercises_Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gyBRvUP6TqmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Week 3 Class Exercises: Training Logistic Regression\n",
        "\n",
        "This week, we are breaking down how training/fitting logistic regression model (our mystery function) works, using our running example of probing bias in word vectors.\n"
      ]
    },
    {
      "metadata": {
        "id": "W8-lxbAqN-rT",
        "colab_type": "code",
        "outputId": "89b5e4d1-c85a-4827-d631-92bfe3a6d325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torchtext.vocab as vocab\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "# Download class resources...\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit2_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "VEC_SIZE = 300\n",
        "glove = vocab.GloVe(name='6B', dim=VEC_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:16, 2.29MB/s]                          \n",
            "100%|█████████▉| 399196/400000 [00:42<00:00, 9565.27it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "spTtKe1sWqZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's what we introduced the first week. Review it and make sure you still understand it!"
      ]
    },
    {
      "metadata": {
        "id": "4fAik-FHWsU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "    return glove.vectors[glove.stoi[word]].numpy()\n",
        "  \n",
        "def compute_cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "  \n",
        "man = get_word_vector('man')\n",
        "woman = get_word_vector('woman')\n",
        "gender_vector = woman - man\n",
        "\n",
        "def compute_linear_regression(word):\n",
        "    word_vector = get_word_vector(word)\n",
        "    return np.dot(gender_vector, word_vector)\n",
        "  \n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-z))\n",
        "  \n",
        "def compute_logistic_regression(word, weights, bias):\n",
        "    word_vector = get_word_vector(word)\n",
        "    return sigmoid(np.dot(weights, word_vector) + bias)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YgU2z_e_ZOJF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "\n",
        "We need to calculate loss. As mentioned in lecture, the loss is a way to quantify how badly we are doing at making predictions. Once we have an idea of how well we are doing, we can create a model that minimizes loss. Finish the below function for calculating loss, which is a function of $y$ and $\\hat{y}$:"
      ]
    },
    {
      "metadata": {
        "id": "TRJKJKlAcI4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_loss(y, y_hat):\n",
        "  ### YOUR CODE HERE ###\n",
        "  return -1 * (1 - y) * np.log(1 - y_hat) - y * np.log(y_hat)\n",
        "  ### END CODE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ANdd5_D7cX3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "\n",
        "We need to calculate the gradients with respect to the weights and bias to move the weights and bias in the direction opposite the gradient (which minimizes loss). This is called gradient descent. Finish the below functions for calculating loss. Note that you might not need all of $y$, $\\hat{y}$, and word for calculating both gradients."
      ]
    },
    {
      "metadata": {
        "id": "XT6lQtsodBgY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_weight_gradient(y, y_hat, word):\n",
        "  ### YOUR CODE HERE ###\n",
        "  return get_word_vector(word) * (y_hat - y)\n",
        "  ### END CODE ###\n",
        "  \n",
        "def get_bias_gradient(y, y_hat, word):\n",
        "  ### YOUR CODE HERE ###\n",
        "  return y_hat - y\n",
        "  ### END CODE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryQDW19Xdgaf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3"
      ]
    },
    {
      "metadata": {
        "id": "DlcSF2uUTqmK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How do we actually train *weights* and *bias*? For our first and only \"random guess\", we can initialize *weights* randomly and *bias* as 0. We then update both of these away from the direction of the gradient with respect to loss, for each training example. Since the gradient is the direction of maximum 'upward slope', moving away from the gradient minimizes the loss. Note that we loop over the training set *NUM_EPOCHS*=1000 times so that the model has more time to learn the training set.\n",
        "\n",
        "You will need *np.random.randn* (see previous cell for usage example), *np.log* (to calculate loss), and our helper functions. \n",
        "\n",
        "Don't worry if you don't finish this in class, we expect this!\n",
        "\n",
        "**Some hints:**\n",
        "\n",
        "Initialize *weights* using *np.random.randn* and bias to 0.\n",
        "\n",
        "You are first computing the prediction *pred* (or *y_hat*), then using this to compute the loss, then computing the gradients, then using them to make weight updates.\n",
        "\n",
        "When computing loss, accumulate loss over each epoch using the '+=' operator, so the final loss printed per epoch is the sum of the losses for each training example.\n",
        "\n",
        "Notation note: dw and db are short for the partial derivatives of the loss with respect to w and b, respectively.\n",
        "\n",
        "Use the *LEARNING_RATE* provided to update the parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "k_d6Vw6PN-s_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_logistic_regression(training_data, NUM_EPOCHS=1000, LEARNING_RATE=0.001):\n",
        "    np.random.seed(42)\n",
        "    # YOUR CODE HERE - initialize weights and bias\n",
        "    weights = np.random.randn(VEC_SIZE) \n",
        "    bias = 0\n",
        "    # END CODE\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = 0\n",
        "        for example in training_data:\n",
        "            x, y = example\n",
        "            # YOUR CODE HERE\n",
        "            y_hat = compute_logistic_regression(x, weights, bias)\n",
        "            loss += get_loss(y, y_hat)\n",
        "            \n",
        "            db = get_bias_gradient(y, y_hat, x)\n",
        "            dw = get_weight_gradient(y, y_hat, x)\n",
        "            \n",
        "            weights -= LEARNING_RATE * dw\n",
        "            bias -= LEARNING_RATE * db\n",
        "            # END CODE\n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Epoch %d, loss = %f\" % (epoch, loss))   \n",
        "    return weights, bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvZu5I8ZN-tD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By looping through each test example and computing gradients for each individually, we are performing what is called Stochastic Gradient Descent (SGD). We'll learn more about this in the coming weeks.\n",
        "\n",
        "Test your implementation to see if its results match ours:"
      ]
    },
    {
      "metadata": {
        "id": "e9_hoCLJN-tE",
        "colab_type": "code",
        "outputId": "d865af2f-7544-4de6-dbca-2ed710d83f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "toy_examples = [('boy', 0), ('girl', 1)]\n",
        "weights, bias = fit_logistic_regression(toy_examples)\n",
        "print(\"First value in weights is %f\" % weights[0])\n",
        "print(\"Bias is %f\" % bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss = 3.843859\n",
            "Epoch 100, loss = 1.420667\n",
            "Epoch 200, loss = 1.006021\n",
            "Epoch 300, loss = 0.816709\n",
            "Epoch 400, loss = 0.680780\n",
            "Epoch 500, loss = 0.578991\n",
            "Epoch 600, loss = 0.500879\n",
            "Epoch 700, loss = 0.439593\n",
            "Epoch 800, loss = 0.390541\n",
            "Epoch 900, loss = 0.350585\n",
            "First value in weights is 0.396410\n",
            "Bias is 0.083128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5S93FYJYN-tJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "Epoch 0, loss = 3.843859\n",
        "\n",
        "Epoch 100, loss = 1.420667\n",
        "\n",
        "Epoch 200, loss = 1.006021\n",
        "\n",
        "Epoch 300, loss = 0.816709\n",
        "\n",
        "Epoch 400, loss = 0.680780\n",
        "\n",
        "Epoch 500, loss = 0.578991\n",
        "\n",
        "Epoch 600, loss = 0.500879\n",
        "\n",
        "Epoch 700, loss = 0.439593\n",
        "\n",
        "Epoch 800, loss = 0.390541\n",
        "\n",
        "Epoch 900, loss = 0.350585\n",
        "\n",
        "First value in weights is 0.396410\n",
        "\n",
        "Bias is 0.083128\n",
        "\n",
        "You've just built a working logistic regression model from scratch. This is actually equivalent to a 1-layer neural network! See that this produces the same results as before when trained on our full data:"
      ]
    },
    {
      "metadata": {
        "id": "mD-3gLDIalqM",
        "colab_type": "code",
        "outputId": "0cf7e65f-36b6-4478-a798-c6645f6b38d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "def read_train_examples():\n",
        "    with open('unit2_resources/train.txt', 'r') as f:\n",
        "        raw_text = f.read()\n",
        "        lines = raw_text.split('\\n')\n",
        "        examples = [line.split() for line in lines]\n",
        "        examples = [(line[0], int(line[1])) for line in examples]\n",
        "    return examples\n",
        "  \n",
        "\n",
        "examples = read_train_examples()  \n",
        "weights, bias = fit_logistic_regression(examples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss = 127.387626\n",
            "Epoch 100, loss = 21.641045\n",
            "Epoch 200, loss = 8.226585\n",
            "Epoch 300, loss = 4.346964\n",
            "Epoch 400, loss = 2.740866\n",
            "Epoch 500, loss = 1.956233\n",
            "Epoch 600, loss = 1.521469\n",
            "Epoch 700, loss = 1.250360\n",
            "Epoch 800, loss = 1.065492\n",
            "Epoch 900, loss = 0.931076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HIYewmFEb1KE",
        "colab_type": "code",
        "outputId": "471cd74b-d143-406a-fbbe-221f51310669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "def print_test_output(test_examples, weights, bias):\n",
        "    for test_example in test_examples:\n",
        "        pred = compute_logistic_regression(test_example, weights, bias)\n",
        "        print(\"%s is %s\" % (test_example, 'male' if pred < .5 else 'female'))\n",
        "        \n",
        "print_test_output(['nurse', 'homemaker', 'carpenter', 'surgeon', 'doctor', 'artist', \n",
        "                   'engineer', 'entrepreneur', 'genius', 'intellectual', 'chef', 'cook', \n",
        "                   'maid', 'teacher', 'boss', 'manager', 'founder'], weights, bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nurse is female\n",
            "homemaker is female\n",
            "carpenter is male\n",
            "surgeon is male\n",
            "doctor is male\n",
            "artist is female\n",
            "engineer is male\n",
            "entrepreneur is male\n",
            "genius is male\n",
            "intellectual is male\n",
            "chef is male\n",
            "cook is female\n",
            "maid is female\n",
            "teacher is female\n",
            "boss is male\n",
            "manager is male\n",
            "founder is male\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qq18Y_ru3CBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Try playing around with NUM_EPOCHS and LEARNING_RATE to see how this affects the loss values printed during training. These values, which are not learned during training but impact the performance of the model, are called hyperparameters. We'll see more examples of hyperparameters in coming weeks.\n",
        "\n",
        "Congratulations on completing the notebook–you learned how to train a complete 1-layer neural network! For homework, we'll see that the tools you just built are useful beyond word vectors; your code naturally extends to a social good problem in a completely different domain."
      ]
    },
    {
      "metadata": {
        "id": "joil_pAR4FGK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}