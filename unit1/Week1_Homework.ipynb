{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week1_Homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dyyqnba3ORAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Week 1 Homework\n",
        "\n",
        "In this homework, we will apply the logistic regression model we built in class and see that gender biases transfer from the word vectors to our complete model. All we will do is tell the model that 1 is for female and 0 is for male, and it will learn harmful gender stereotypes (from the word vectors, which in turn learned from Wikipedia, a dataset containing implicit bias).\n",
        "\n",
        "This is different from the linear regression case because we are training a model that is making predictions on the word vector, rather than fixing the weights as some vector. This means that gender biases in word vectors are so prevalent that any model built on top of them (just about any model for natural language processing) is at risk for including implicit bias. "
      ]
    },
    {
      "metadata": {
        "id": "9XibaWo4ORAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchtext.vocab as vocab\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "np.random.seed(42)\n",
        "# Download class resources...\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit1_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "VEC_SIZE = 300\n",
        "glove = vocab.GloVe(name='6B', dim=VEC_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h81IZUl5ORA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below we define a couple of helpful helper functions, including our *get_word_vector* from before."
      ]
    },
    {
      "metadata": {
        "id": "YJA0JxjYORA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "    return glove.vectors[glove.stoi[word]].numpy()\n",
        "  \n",
        "def read_train_examples():\n",
        "    with open('unit1_resources/train.txt', 'r') as f:\n",
        "        raw_text = f.read()\n",
        "        lines = raw_text.split('\\n')\n",
        "        examples = [line.split() for line in lines]\n",
        "        examples = [(line[0], int(line[1])) for line in examples]\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pb6cMa7oORBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, copy-and-paste your complete *sigmoid* and *compute_logistic_regression* functions from the in-class exercises. If you didn't have time in class to finish these functions, you should do that now. We will use them below."
      ]
    },
    {
      "metadata": {
        "id": "J1ZPvkMMORBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    # YOUR CODE HERE\n",
        "    return None\n",
        "    # END CODE\n",
        "\n",
        "def compute_logistic_regression(word, weights, bias):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return None\n",
        "    # END CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzbPEkDxdNfx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below we include a special helper function, *fit_logistic_regression*, that automatically selects weights and bias given a training data set using gradient descent. Don't worry about the implementation just yet (it is deliberately hidden from you)â€“we'll see where this all comes from in our next lecture. For now, you can view this function as a black box that chooses weights and bias such that the resulting model makes predictions that look like the training data. That is, the weights and bias are chosen so that 1 is for female and 0 is for male."
      ]
    },
    {
      "metadata": {
        "id": "iaW-AHU5evZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_logistic_regression (O000O0OO0OOOOO000 ,OOO000O0O000OOO00 =1000 ,O00O0O0OOO00O00OO =0.001 ):\n",
        "    np .random .seed (42 )\n",
        "    O0OOO0OO0O0O0OO00 =np .random .randn (VEC_SIZE )\n",
        "    O0OOO00O0OOOOO000 =0 \n",
        "    for OO0O0000O000OOO00 in range (OOO000O0O000OOO00 ):\n",
        "        O0OOOO0OO0OO00000 =0 \n",
        "        for O00OOO0OO0000OO0O in O000O0OO0OOOOO000 :\n",
        "            OO0O00O000OOO0O00 ,O00OO0O0O00OO0000 =O00OOO0OO0000OO0O \n",
        "            O0O0OO0000OOO0O0O =compute_logistic_regression (OO0O00O000OOO0O00 ,O0OOO0OO0O0O0OO00 ,O0OOO00O0OOOOO000 )\n",
        "            O0OOOO0OO0OO00000 +=(1 -O00OO0O0O00OO0000 )*np .log (1 -O0O0OO0000OOO0O0O )+O00OO0O0O00OO0000 *np .log (O0O0OO0000OOO0O0O )\n",
        "            O0OOO0O0OO00O0O0O =O0O0OO0000OOO0O0O -O00OO0O0O00OO0000 \n",
        "            OOO00OO0O0OOOO000 =O0OOO0O0OO00O0O0O\n",
        "            OOOOOO0OOOOO0OO0O =get_word_vector (OO0O00O000OOO0O00 )*O0OOO0O0OO00O0O0O\n",
        "            O0OOO0OO0O0O0OO00 -=O00O0O0OOO00O00OO *OOOOOO0OOOOO0OO0O\n",
        "            O0OOO00O0OOOOO000 -=O00O0O0OOO00O00OO *OOO00OO0O0OOOO000\n",
        "        if OO0O0000O000OOO00 %100 ==0 :\n",
        "            print (\"Epoch %d, loss = %f\"%(OO0O0000O000OOO00 ,O0OOOO0OO0OO00000 ))\n",
        "    return O0OOO0OO0O0O0OO00 ,O0OOO00O0OOOOO000 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-KbE6yOORBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, use *read_train_examples* to read examples from 'resources/train.txt'. Browse the training examples and note that each contains a word with an appropriate, non-problematic gender association. By training our model on these words, we are effectively telling it that 'female' should result in a '1' output and 'male' should result in a '0' output."
      ]
    },
    {
      "metadata": {
        "id": "Ej2n3UlfORBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "examples = None\n",
        "### END CODE\n",
        "print(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZY18LbSZORBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then use *fit_logistic_regression* to get weights and a bias for this data. The first return argument is weights, and the second is bias. We will use these to make predictions on unseen data below."
      ]
    },
    {
      "metadata": {
        "id": "4N8p2_d8ORBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "weights, bias = None\n",
        "### END CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbltNp6GORBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected output**:\n",
        "\n",
        "Epoch 0, loss = -127.387626\n",
        "\n",
        "Epoch 100, loss = -21.641045\n",
        "\n",
        "Epoch 200, loss = -8.226585\n",
        "\n",
        "Epoch 300, loss = -4.346964\n",
        "\n",
        "Epoch 400, loss = -2.740866\n",
        "\n",
        "Epoch 500, loss = -1.956233\n",
        "\n",
        "Epoch 600, loss = -1.521469\n",
        "\n",
        "Epoch 700, loss = -1.250360\n",
        "\n",
        "Epoch 800, loss = -1.065492\n",
        "\n",
        "Epoch 900, loss = -0.931076\n",
        "\n",
        "Below, we define a helper function that \"pretty-prints\" the outputs of our model. Make sure you understand how it works."
      ]
    },
    {
      "metadata": {
        "id": "sW0xJvcFORBq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_test_output(test_examples, weights, bias):\n",
        "    for test_example in test_examples:\n",
        "        pred = compute_logistic_regression(test_example, weights, bias)\n",
        "        print(\"%s is %s\" % (test_example, 'male' if pred < .5 else 'female'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC0Q5CmHORBv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a sanity check, let's see what happens if we print output for pronouns that were in the training data. Testing on train data is a common technique to debug models and make sure everything is working properly."
      ]
    },
    {
      "metadata": {
        "id": "PCmj5ol5ORBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_test_output(['she', 'he'], weights, bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8_q6yhFORB2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected output:**\n",
        "\n",
        "she is female\n",
        "\n",
        "he is male\n",
        "\n",
        "Now, let's see what happens if we ask the model about common professions. Note that we did not tell the model about these gender-neutral professions, but it is able to make predictions on them anyway because we still have word vectors for them. This is called \"zero-shot\" learning, and it is possible because the dataset from which we derived the word vectors contains these words."
      ]
    },
    {
      "metadata": {
        "id": "q1JnNyTMORB3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_test_output(['nurse', 'homemaker', 'carpenter', 'surgeon', 'doctor', 'artist', \n",
        "                   'engineer', 'entrepreneur', 'genius', 'intellectual', 'chef', 'cook', \n",
        "                   'maid', 'teacher', 'boss', 'manager', 'founder'], weights, bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwkkQMnbORB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the model seems to capture common gender stereotypes about professions. We never explicitly told the model any of these things during training!\n",
        "\n",
        "This is alarming, since almost all natural language processing models are built on top of these word vectors or ones like them. If we can accidentally built a model that displays gender bias this easily, so can people working at Google, Facebook, Microsoft, etc. We will continue our exploration of bias in word vectors during week 3 and learn how to implement *fit_logistic_regression*. In the meantime, if you're interested in looking at techniques to debias word vectors, [here is a great, approachable paper on just that](https://arxiv.org/abs/1607.06520)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ju1ib3cfdJHd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}