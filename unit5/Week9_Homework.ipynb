{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week9_Homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBGYEr-NgM1y",
        "colab_type": "text"
      },
      "source": [
        "# Week 9 Homework: Fake News Evaluation\n",
        "\n",
        "For homework, we will build off of the class exercises. One thing you might have noticed as you were looking at text generated by our language model was that it was difficult to tell how well the language model was doing besides eye-balling the quality of the generated text. It turns out there is a way to quantitatively evaluate the model, using a metric called perplexity. Load the following cell to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj78oVeJgM11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import load_model\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Download and extract data.\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit5_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMfP1IquJ2Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to load helpers (double-click to read code) { display-mode: \"form\" }\n",
        "\n",
        "def get_X_y(text):\n",
        "\ttext = text.lower()\n",
        "\ttext = simplify_text(text)\n",
        "\n",
        "\tprint('Corpus length:', len(text))\n",
        "\n",
        "\tchars = sorted(list(set(text)))\n",
        "\tprint('Total chars:', len(chars))\n",
        "\tchar_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "\tindices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\t# cut the text in semi-redundant chunks of maxlen characters\n",
        "\tmaxlen = 40\n",
        "\tstep = 3\n",
        "\tsentences = []\n",
        "\tnext_chars = []\n",
        "\tfor i in range(0, len(text) - maxlen, step):\n",
        "\t    sentences.append(text[i: i + maxlen])\n",
        "\t    next_chars.append(text[i + maxlen])\n",
        "\tprint('Chunk length:', maxlen)\n",
        "\tprint('Number of chunks:', len(sentences))\n",
        "\n",
        "\tx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "\ty = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\tfor i, sentence in enumerate(sentences):\n",
        "\t    for t, char in enumerate(sentence):\n",
        "\t        x[i, t, char_indices[char]] = 1\n",
        "\t    y[i, char_indices[next_chars[i]]] = 1\n",
        "\treturn x, y, char_indices, indices_char\n",
        "\n",
        "def simplify_text(text):\n",
        "    counts = defaultdict(int)\n",
        "    for ch in text:\n",
        "        counts[ch] += 1\n",
        "    counts = [(counts[k], k) for k in counts.keys()]\n",
        "    removed = 0\n",
        "    for count, ch in counts:\n",
        "        if count <= 200:\n",
        "            text = text.replace(ch, '')\n",
        "            removed += 1\n",
        "    return text\n",
        "\n",
        "def sample_from_model(model, text, char_indices, indices_char, chunk_length, number_of_characters, seed=\"\"):\n",
        "\ttext = text.lower()\n",
        "\tstart_index = random.randint(0, len(text) - chunk_length - 1)\n",
        "\tfor diversity in [0.2, 0.5, 0.7]:\n",
        "\t    print('----- diversity:', diversity)\n",
        "\n",
        "\t    generated = ''\n",
        "\t    if not seed:\n",
        "\t    \tsentence = text[start_index: start_index + chunk_length]\n",
        "\t    else:\n",
        "\t    \tseed = seed.lower()\n",
        "\t    \tsentence = seed[:chunk_length]\n",
        "\t    \tsentence = ' ' * (chunk_length - len(sentence)) + sentence\n",
        "\t    generated += sentence\n",
        "\t    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "\t    sys.stdout.write(generated)\n",
        "\n",
        "\t    for i in range(400):\n",
        "\t        x_pred = np.zeros((1, chunk_length, number_of_characters))\n",
        "\t        for t, char in enumerate(sentence):\n",
        "\t            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "\t        preds = model.predict(x_pred, verbose=0)[0]\n",
        "\t        next_index = sample(preds, diversity)\n",
        "\t        next_char = indices_char[next_index]\n",
        "\n",
        "\t        generated += next_char\n",
        "\t        sentence = sentence[1:] + next_char\n",
        "\n",
        "\t        sys.stdout.write(next_char)\n",
        "\t        sys.stdout.flush()\n",
        "\t    print(\"\\n\")\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64') + 1e-8\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvyRE_pgM17",
        "colab_type": "text"
      },
      "source": [
        "Run the below cells to load up our dataset and pretrained model as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSbUj_FkgM18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with io.open('unit5_resources/fake.txt', encoding='utf-8') as f:\n",
        "    articles_raw = f.read()\n",
        "    articles_split = re.split(\"<a>\", articles_raw)[1:]\n",
        "    articles = [a[:-6].strip() for a in articles_split]\n",
        "\n",
        "X, y, char_indices, indices_char = get_X_y(articles_raw)\n",
        "\n",
        "print(\"Shapes:\", X.shape, y.shape)\n",
        "\n",
        "number_of_chunks, chunk_length, number_of_characters = X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f3pFigLgM2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model = load_model('unit5_resources/pretrained_model.h5')\n",
        "pretrained_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQRKobkkgM2E",
        "colab_type": "text"
      },
      "source": [
        "We can see that we are working with a 2-layer LSTM. When evaluating models, we generally prefer tune our hyperparameters based on quantitative performance metrics, such as test set accuracy. We don't have anything like that at the moment for our language model, so we will use perplexity. \n",
        "\n",
        "Intuitively, perplexity is a measure of how confused our model is when it looks at our original dataset. For each chunk, we compare our model's predictions to the actual next character, and if the model's predictions differ by a lot, the model has a high perplexity. Thus, we aim to minimize perplexity when building a language model.\n",
        "\n",
        "More formally, perplexity is the geometric mean of the product of the inverse prediction probabilities for the correct characters: \n",
        "\n",
        "![Perplexity](http://web.stanford.edu/class/cs21si/resources/assets/perplexity.png)\n",
        "\n",
        "Note that for us, T is the number of chunks. To calculate perplexity, we evaluate our model on each chunk to produce a y_hat for that chunk. In practice, we may instead calculate the log(Perplexity), which simplifies to the following:\n",
        "\n",
        "![log perplexity](http://web.stanford.edu/class/cs21si/resources/assets/log-perplexity.png)\n",
        "\n",
        "Note that in both of these equations we have the following term:\n",
        "\n",
        "![Probability term](http://web.stanford.edu/class/cs21si/resources/assets/prob-term.png)\n",
        "\n",
        "Does this term look familiar? It is the dot-product of our prediction vector and the one-hot label vector. When we compute the dot product with a one-hot vector, we are effectively doing an index lookup at the position represented by the one-hot vector. This sum always simplifies to the predicted probability of the correct character, but it is still simpler to compute this as a dot product in code.\n",
        "\n",
        "We can see that the perplexity is a natural measure of how well a language model captures its training data. Although we won't do this, we can also see that it might extend to helping us classify language: presumably, if we ask our model to evaluate perplexity on unseen fake news text and real news text, perplexity will be higher for real news. This suggests a naive approach for fake news classification!\n",
        "\n",
        "Now you will finish a function to compute log(perplexity). Note that to compute this, you need all of the prediction vectors for each chunk (this corresponds to the y_hat(t) vectors in the equations above). We have provided the below function to get all of the predictions for our dataset using our pretrained model. The reason this code is more complicated than you'd expect is that Keras expects the input to come as batches of size 128 since this is how we trained our model. Don't worry about the details of this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6mmtC-IgM2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pred(model, X):\n",
        "    number_of_chunks, chunk_length, number_of_characters = X.shape\n",
        "    pred = np.zeros((number_of_chunks, number_of_characters))\n",
        "    num_batches = int(math.ceil(number_of_chunks / 128.0))\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        curr_pred = model.predict(X[(i) * 128: (i + 1) * 128])\n",
        "        pred[(i) * 128: (i + 1) * 128] = curr_pred\n",
        "    return pred\n",
        "\n",
        "print(\"Getting predictions... this will take a few minutes.\")\n",
        "pretrained_pred = get_pred(pretrained_model, X)\n",
        "print(\"\\n\\nDone getting predictions.\")\n",
        "print(\"Shape:\", pretrained_pred.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWFozsFgM2H",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our predictions, we simply need to use them, along with *y*, to compute log(perplexity). You just need to change two lines below to finish the below function. Hint: the first line involves computing the predicted probability of the correct character. You may find *np.dot* useful for this.\n",
        "\n",
        "Note: in the case that the predicted probability of the correct character is exactly 0.0 due to underflow or some other issue, we skip this chunk to avoid log domain errors. This is obviously a hack, but we do it infrequently enough that our metric is still meaningful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ierQXsi7gM2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_log_perplexity(pred, y):\n",
        "    number_of_chunks, number_of_characters = pred.shape\n",
        "    total = 0.0\n",
        "    for t in range(number_of_chunks):\n",
        "        ### YOUR CODE HERE\n",
        "        prob = None\n",
        "        ### END CODE\n",
        "        # If prob is 0.0, skip to avoid log domain errors\n",
        "        if prob == 0.0:\n",
        "            continue\n",
        "        ### YOUR CODE HERE\n",
        "        total -= None\n",
        "        ### END CODE\n",
        "    return total / number_of_chunks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL4FmdehgM2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_log_perplexity(pretrained_pred, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAfhEZyogM2M",
        "colab_type": "text"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "1.1104002202573617"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPlpZ_ABgM2N",
        "colab_type": "text"
      },
      "source": [
        "## Optional: Train your own model and evaluate it!\n",
        "\n",
        "You can also optionally train your own model with your own choice of architecture and evaluate it against our new metric! Go into the [*resources* directory](http://web.stanford.edu/class/cs21si/resources/unit5_resources.zip) and edit *lstm_text_generation.py* to your liking. You should only need to edit lines 93-97 if you want to change the architecture only, but feel free to change anything else you like. Simply run ```python lstm_text_generation.py``` to train for up to 300 epochs and save model files each epoch (this will take a really long time, feel free to stop training at any time). The model files will be saved to the *outputs* directory, which will be created if it does not already exist. You should expect the model to take a few minutes per epoch and around 100 epochs before it has mostly converged on CPU. Once you have a trained model, just insert your model path below and run the cells to evaluate it. See if you can do better than our pretrained model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOKlTsbPgM2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR MODEL PATH HERE\n",
        "your_model = load_model('resources/outputs/lstm_epochXXX.h5')\n",
        "### END MODEL PATH\n",
        "your_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am6JlT2rgM2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "your_pred = get_pred(your_model, X)\n",
        "get_log_perplexity(your_pred, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpxEqXKOgM2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}