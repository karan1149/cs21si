{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week7_Homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUb8vem4BBND",
        "colab_type": "text"
      },
      "source": [
        "# Week 7 Homework: Adversarial Attacks\n",
        "\n",
        "This week, we'll be talking about adversarial examples. Basically, we fool the neural network into thinking that an image which is abnormal is totally healthy--super scary stuff!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGi80EjuBBNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os, scipy.ndimage, scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import util\n",
        "import requests, zipfile, io\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import load_model\n",
        "from keras.objectives import binary_crossentropy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Download and extract data.\n",
        "r = requests.get(\"http://web.stanford.edu/class/cs21si/resources/unit4_resources.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeH_Xx1fBBNK",
        "colab_type": "text"
      },
      "source": [
        "## Load the Model\n",
        "\n",
        "First, we load a pre-trained model on this task and check out the summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak2Eo1AMBBNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('unit4_resources/trained_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCFC9SV2BBNP",
        "colab_type": "text"
      },
      "source": [
        "## Pick a Test Case\n",
        "\n",
        "We now pick an image that we are going to play with. This image is an abnormal sample from the test set--in other words, we pick an image from the test set which shows an unhealthy lession on the skin of the patient. Notice that the output the model predicts is above 0.5, so it thinks that this is unhealthy. We will make the model believe this is healthy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFh5EPvBBNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = 'unit4_resources/addi/test/abnormal/'\n",
        "img_path = base_path + os.listdir(base_path)[3]\n",
        "img = scipy.ndimage.imread(img_path).astype(float)\n",
        "img = scipy.misc.imresize(img, (150, 150, 3))[np.newaxis, :, :, :]/255.0\n",
        "\n",
        "plt.imshow(np.squeeze(img))\n",
        "plt.show()\n",
        "\n",
        "model.predict(img)[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-h8Aw_xBBNS",
        "colab_type": "text"
      },
      "source": [
        "## Untargeted Noise\n",
        "\n",
        "Before we dive into how adversarial examples work, try this out: just intialize some random noise and add it to our above image. Then, see how the model does. Mess around with the scaling/shifting factor of the noise to try to get the score below 0.5 (making the model think it is healthy). This is called an untargeted attack--we don't know what output the model will give this noisy input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqLexUYIBBNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Change the scale and shift to fool the model\n",
        "scale, shift = 0, 0\n",
        "noise = np.random.rand(1, 150, 150, 3) * scale + shift\n",
        "\n",
        "# Add the noise to the image\n",
        "noisy_img = np.clip(img + noise, 0, 1)\n",
        "plt.imshow(np.squeeze(noisy_img))\n",
        "plt.show()\n",
        "\n",
        "# Have the model predict on the image\n",
        "model.predict(noisy_img)[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZJq95qDBBNW",
        "colab_type": "text"
      },
      "source": [
        "## Targeted Noise\n",
        "\n",
        "Now for the interesting stuff: we will change the image until we fool the model. How does this work? It's quite simple: we compute the gradient of the normal class score with respect to the input image. This tells us how much to change the input image to maximize the normal class score--i.e. how do we change the unhealthy image to make it look healthy. That's it!\n",
        "\n",
        "Notice how, as we execute this code to do that, the score decreases. Mess with the pertubation amount to try to get the model output below a 0.5 on this input. This means that the model now thinks that the unhealthy image is healthy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jabw0IsDBBNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pertubation_amount = 0.1\n",
        "def add_targetted_noise(fooling_img):\n",
        "  for _ in range(15):\n",
        "    y_true = K.placeholder((1, 1))\n",
        "    loss = K.mean(binary_crossentropy(y_true, model.output))\n",
        "    get_grads = K.function([model.input, y_true], K.gradients(loss, model.input))\n",
        "\n",
        "    grad = get_grads([fooling_img, [[0]]])[0]\n",
        "    fooling_img = fooling_img - pertubation_amount*grad\n",
        "    \n",
        "    pred = model.predict(fooling_img)[0, 0]\n",
        "    if pred <= 0.5:\n",
        "        break\n",
        "  \n",
        "  return fooling_img\n",
        "\n",
        "\n",
        "fooling_img = add_targetted_noise(img.copy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq3NLjj9BBNZ",
        "colab_type": "text"
      },
      "source": [
        "## Visualize the \"Fooling\" Image\n",
        "\n",
        "Let's see how this adversarial example looks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY02VtnfBBNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(np.squeeze(fooling_img))\n",
        "plt.show()\n",
        "pred = model.predict(fooling_img)[0, 0]\n",
        "print(\"Prediction using this image: \", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kleMXFzoBBNe",
        "colab_type": "text"
      },
      "source": [
        "Wow, this looks exactly like the input image! However, our model thinks that this is healthy when it is clearly not. In just a couple of steps, you were able to take an unhealthy image and make it look healthy to the network. That is super scary! This is something to keep in mind as you design your high-stake ML algorithms.\n",
        "\n",
        "However, there is something you can do to help mitigate this issue: train on images that are perturbed in such a way. Let's do this now by creating fooling images out of all abnormals and then training on them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxyh-W9pl6z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fooling_batch = []\n",
        "for i in range(10):\n",
        "  img_path = base_path + os.listdir(base_path)[i]\n",
        "  img = scipy.ndimage.imread(img_path).astype(float)\n",
        "  img = scipy.misc.imresize(img, (150, 150, 3))[np.newaxis, :, :, :]/255.0\n",
        "  \n",
        "  fooling_img = add_targetted_noise(img.copy())\n",
        "  fooling_batch.append(fooling_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPBNUqWoYqh",
        "colab_type": "text"
      },
      "source": [
        "Let's test how the model does on the last 2 out of 10 examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FudKjh6NuZ-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(np.array(fooling_batch)[8:].squeeze(), [1 for _ in range(2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDmnJg_zoiY8",
        "colab_type": "text"
      },
      "source": [
        "It fails for both of them! Now, fine-tune the model (further train a model that has already been trained) on the first 8 out of 10 images and then see how it does on the final 2. Hint: call *model.train_on_batch*!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-58a1qgBBNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a fresh model so you can play around with this cell many times\n",
        "model = load_model('unit4_resources/trained_model.h5')\n",
        "\n",
        "# Fine-Tune the Model\n",
        "def fine_tune(images, labels):\n",
        "  \n",
        "  for _ in range(50):\n",
        "    ### YOUR CODE HERE ### (1 line)\n",
        "    \n",
        "    ### END CODE ###\n",
        "\n",
        "fine_tune(np.array(fooling_batch[:8]).squeeze(), [1 for _ in range(8)])\n",
        "    \n",
        "# Evaluate on the final 2 examples\n",
        "model.evaluate(np.array(fooling_batch)[8:].squeeze(), [1 for _ in range(2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcDxtvRossO",
        "colab_type": "text"
      },
      "source": [
        "So we see that training on adversarial examples is a good way to protect our models against them.\n",
        "\n",
        "If you want to know more about adversarial examples and how we can protect against them, check out [this paper on the topic](https://arxiv.org/pdf/1712.07107.pdf). For a more casual read, check out [this neat blog post](https://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/) on the topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzDRCToIzeXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}